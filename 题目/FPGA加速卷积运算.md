# FPGA 神经网络加速器的加速原理

##   基本原理

神经网络是一种基于大脑神经网络的机器学习模型。一系列节点排列在“层”中，通过操作和权重相互连接。该模型已证明在图像分类任务中取得了成功，这些任务如今具有许多应用，从自动驾驶汽车到面部识别。标准 CNN 可以具有浮点权重和特征图——这些需要大量的内存资源和计算能力来实现必要的乘法器等。

目前卷积神经网络的FPGA知加速研究主要集中在**并行计算**和**内存带宽优化**两方面，其中并行计算主要通过设计卷积层间并行、卷积内计算并行和输出通道并行3种方式来实现加速免，此类单纯的硬件并行加速方法资源占用较多、带宽需求较大，实际应用中仍需做相应的改进。内存带宽优化通常采用一些优化算法定量分析计算吞吐量和所需内存的带宽，确定最佳性能进币解决资源占用量大的问题，此类方案在不同层间需要重新配置，灵活性稍显不足。

在加速算法没计方面，一般采用通用用阵乘法算法将矩阵转换为向量，并对每个向量一对一计算，最后将向量计算结果转换为矩阵但并未减少计算量，同时又产生大量的读写需求和内存需求。

## 并行计算



## 内存带宽优化

##  移位寄存器

<img src="C:\Users\26825\Desktop\集创赛\校内初赛\题目\picture\shift_ram.jpg" style="zoom: 25%;" />
